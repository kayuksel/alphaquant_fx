# AlphaEvolve for Discovery of Forex Trading Strategies

## Overview

This repository implements a novel, adaptive technical indicator discovered through LLM-driven scientific discovery. The indicator synthesizes multi-horizon momentum, volatility regime detection, tail-risk adjustment, and Gaussian-weighted trend filtering to generate a normalized trading signal for the AUDNZD currency pair on hourly data. The parameters of the discovered trading strategy are not tuned.

## Results

Below equity curve generated by applying the discovered strategy to AUDNZD hourly data (rolling windows of length=200 are fed to the below technical_indicator function to generate signals, and then positions are taken by thresholding values generated by the technical indicator.)

![Strategy Results](strategy_result.png)

## Technical Indicator Code

```python
def technical_indicator(ohlcv: torch.Tensor, eps = 1e-06) -> torch.Tensor:
    (n, T, f) = ohlcv.shape
    pad = lambda x, L: x if x.size(1) >= L else F.pad(x, (L - x.size(1), 0), mode='replicate')
    log_close = ohlcv[..., 3].clamp_min(eps).log()
    rts = log_close - torch.cat([log_close[:, :1], log_close[:, :-1]], dim=1)
    base_win = 20
    vol_recent = pad(rts, base_win)[:, -base_win:].std(dim=1)
    adapt = (1 + 0.2 * ((vol_recent - vol_recent.median()) / (vol_recent.median() + eps))).clamp(0.8, 1.2)
    multi_fd = torch.stack([rts[:, -1] - pad(rts, w)[:, -w] for w in [5, 10, 20]], dim=0).mean(dim=0).tanh()
    roll = pad(rts, base_win).unfold(1, base_win, 1)
    entropy = (1 + roll.std(dim=2)).log().mean(dim=1).sigmoid()
    vs = pad(ohlcv[..., 4], 60)
    dv = vs[:, 1:] - vs[:, :-1]
    avg_gain = dv.clamp_min(0).unfold(1, 14, 1).mean(dim=2)[:, -1]
    avg_loss = (-dv).clamp_min(0).unfold(1, 14, 1).mean(dim=2)[:, -1].clamp_min(eps)
    vol_exp = (vs.std(dim=1) / (vs.mean(dim=1) + eps)).clamp(0.8, 1.2)
    rsi_norm = ((100 - 100 / (1 + avg_gain / avg_loss)) / 100) ** vol_exp
    vol_adj = (vs[:, -1] / (vs.mean(dim=1) + eps)).sigmoid() * pad(vs, 14).unfold(1, 14, 1).std(dim=2)[:, -1].sigmoid()
    ens_wins = torch.tensor([10, 30, 60], device=rts.device, dtype=torch.long)
    rp2 = pad(rts, int(ens_wins.max().item()))
    blend = lambda x: 0.3 * x.quantile(0.25, dim=1) + 0.4 * x.quantile(0.5, dim=1) + 0.3 * x.quantile(0.75, dim=1)
    (S_list, W_list) = ([], [])
    for w in ens_wins.tolist():
        roll_win = rp2.unfold(1, w, 1)
        last_roll = roll_win[:, -1, :]
        wk = torch.exp(-0.5 * ((torch.arange(w, device=rts.device, dtype=rts.dtype) - (w - 1)) / (w * 0.5 + eps)) ** 2)
        moment = (last_roll * wk / (wk.sum() + eps)).sum(dim=1)
        m_last = last_roll.mean(dim=1)
        std_last = last_roll.std(dim=1).clamp_min(eps)
        skew = ((last_roll - m_last.unsqueeze(1)) ** 3).mean(dim=1) / std_last ** 3
        kurt = ((last_roll - m_last.unsqueeze(1)) ** 4).mean(dim=1) / std_last ** 4 - 3
        q_val = last_roll.quantile(0.05, dim=1)
        msk = last_roll < q_val.unsqueeze(1)
        cvar_est = last_roll.masked_fill(~msk, 0).sum(dim=1) / msk.sum(dim=1).clamp_min(1)
        tail = (-(cvar_est - q_val) * (1 + torch.relu(-skew))).sigmoid()
        comb = entropy * moment + (1 - entropy) * blend(last_roll) 
        comb += 0.1 * (skew.tanh() - kurt.tanh() + rsi_norm + multi_fd.tanh())
        comb = comb / (1 + 0.5 * vol_recent)
        S_list.append((-(comb * vol_adj) ** 2).exp() * tail * multi_fd.tanh().sigmoid())
        W_list.append(moment.sigmoid() / (std_last ** 2 + eps))
    dyn_w = (-torch.stack(W_list, dim=0)).softmax(dim=0)
    multi_ens = (torch.stack(S_list, dim=0) * dyn_w).sum(dim=0)
    weights = torch.arange(rp2.size(1), device=rts.device, dtype=rts.dtype).unsqueeze(0)
    wei_num = 0.7 * (-adapt.unsqueeze(1) * weights).exp() + 0.3 * (-0.05 * weights).exp()
    wei = wei_num / (wei_num.sum(dim=1, keepdim=True) + eps)
    trend = (-10 * ((rp2 * wei).sum(dim=1) - rp2[:, -1]) ** 2).exp() 
    cross = (-(8 * (rp2[:, -1] - rp2[:, -1].median()) / (rts.std(dim=1) + eps))).sigmoid()
    return (multi_ens * trend * cross + eps).pow(1 / 3) * (1 + 0.1 * multi_fd)
```

## Strategy Description

1. **Volatility Regime Detection**

   - Measures recent volatility over a rolling window (up to 20 periods) and benchmarks against the cross-sectional median.
   - Classifies each asset into high/low volatility regimes to adapt weighting and memory parameters.

2. **Adaptive Decay Parameter**

   - Calculates `d_param` by combining standardized volatility deviation and regime flag.
   - Controls the exponential decay rates used when weighing past returns, emphasizing recent observations under high-volatility regimes.

3. **Multi-Window Momentum with Tail Risk**

   - Evaluates momentum across windows from 10 to 60 hours using exponential decay weights to focus on more recent returns.
   - Computes Conditional Value at Risk (5% CVaR) for each window to quantify downside risk.
   - Blends momentum and tail risk via an exponential penalty on extreme moves, then aggregates across windows using inverse-variance weighting.

4. **Gaussian-Weighted Trend Filters**

   - Applies three Gaussian kernels (fast, medium, slow) over the full return history to isolate trend bifurcations.
   - Bandwidths adapt dynamically based on volatility regime and return skewness, capturing asymmetric market conditions.

5. **Cross-Sectional Skew Adjustment**

   - Generates a ranking signal by comparing the latest return to the median of all assets (here only AUDNZD).
   - Scales signal by asset skewness to account for asymmetric return distributions.

6. **Aggregation & Normalization**

   - Multiplies the multi-window momentum, trend-filter score, and cross-sectional rank signals.
   - Applies a cubic root to control dispersion, then transforms via a sigmoid with a steepness factor of 10 to map into [0,1].

7. **Application to AUDNZD Hourly Data**

   - Hourly backtest shows stable equity growth with controlled drawdowns.
   - Captures both mean-reversion in low-volatility periods and trend-following in breakout regimes.
