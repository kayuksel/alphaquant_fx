# LLM-Driven Discovery of Forex Trading Strategies

## Overview

This repository implements a novel, adaptive technical indicator discovered through an LLM-driven scientific discovery workflow. The indicator synthesizes multi-horizon momentum, volatility regime detection, tail-risk adjustment, and Gaussian-weighted trend filtering to generate a normalized trading signal for the AUDNZD currency pair on hourly data. The parameters of the discovered trading strategy are not tuned.

## Results

Below is the equity curve generated by applying the discovered strategy to AUDNZD hourly data (rolling windows of length=200 are fed to the below technical_indicator function to generate signals, and then positions are taken by thresholding values generated by the technical indicator.)

![Strategy Results](strategy_result.png)

## Technical Indicator Code

```python
import torch
import torch.nn.functional as F

def technical_indicator(ohlcv: torch.Tensor) -> torch.Tensor:
    eps = 1e-06
    if ohlcv.size(0) == 0:
        return torch.zeros(0, device=ohlcv.device)
    (n_assets, T, feat) = ohlcv.shape
    close = ohlcv[..., 3]
    log_close = torch.log(close + eps)
    returns = log_close - torch.cat([log_close[:, :1], log_close[:, :-1]], dim=1)
    min_req = 15
    if T < min_req:
        pad_amt = min_req - T
        returns = F.pad(returns, (pad_amt, 0), mode='replicate')
        T = returns.size(1)
    med_filter_win = 3 if T >= 3 else T
    if T >= med_filter_win:
        unfolds = returns.unfold(1, med_filter_win, 1)
        filtered = unfolds.median(dim=-1).values
        pad_left = med_filter_win - 1
        noise_filtered = F.pad(filtered, (pad_left, 0), mode='replicate')
    else:
        noise_filtered = returns
    base_win = 20 if T >= 20 else T
    robust_vol = (returns[:, -base_win:] - returns[:, -base_win:].median(dim=1, keepdim=True)[0]).abs().median(dim=1)[0] / 0.6745 + eps
    vol_med = robust_vol.median()
    vol_spread = robust_vol.abs().median() + eps
    regime = torch.sigmoid((robust_vol - vol_med) / vol_spread)
    d_param = torch.clamp(0.5 + 0.2 * (robust_vol - vol_med) / vol_spread + regime * 0.1, 0.1, 1.0)
    if feat >= 5:
        vol_series = ohlcv[..., 4]
        if T < 15:
            pad_amt = 15 - T
            vol_series = F.pad(vol_series, (pad_amt, 0), mode='replicate')
        delta_vol = vol_series[:, 1:] - vol_series[:, :-1]
        gain = torch.clamp(delta_vol, min=0)
        loss = torch.clamp(-delta_vol, min=0)
        avg_gain = F.avg_pool1d(gain.unsqueeze(1), kernel_size=14, stride=1).squeeze(1)[:, -1]
        avg_loss = F.avg_pool1d(loss.unsqueeze(1), kernel_size=14, stride=1).squeeze(1)[:, -1] + eps
        rsi = 100 - 100 / (1 + avg_gain / avg_loss)
        rsi_norm = rsi / 100.0
    else:
        rsi_norm = 0.5 * torch.ones(n_assets, device=ohlcv.device, dtype=returns.dtype)
    acc = (noise_filtered[:, -1] - noise_filtered[:, -5]) / 4.0 if T >= 5 else noise_filtered[:, -1] - noise_filtered[:, -2]
    min_win = 10
    max_win = 60 if T >= 60 else T
    wins = torch.arange(min_win, max_win + 1, device=returns.device)
    missing = max_win - T
    returns_pad = F.pad(returns, (missing, 0), mode='replicate') if missing > 0 else returns
    sig_list = []
    wt_list = []
    for w in wins.tolist():
        unr = returns_pad.unfold(1, w, 1)
        last_unr = unr[:, -1, :]
        t_lin = torch.arange(w, device=returns.device, dtype=returns.dtype)
        decay = torch.exp(-d_param.unsqueeze(1) * (w - 1 - t_lin))
        time_weights = decay / (decay.sum(dim=1, keepdim=True) + eps)
        moment = (last_unr * time_weights).sum(dim=1)
        window_mad = (last_unr - last_unr.median(dim=1, keepdim=True)[0]).abs().median(dim=1)[0] / 0.6745 + eps
        m = last_unr.median(dim=1, keepdim=True)[0]
        robust_skew = ((last_unr - m) ** 3).median(dim=1)[0] / (window_mad ** 3 + eps)
        robust_kurt = ((last_unr - m) ** 4).median(dim=1)[0] / (window_mad ** 4 + eps) - 3
        q = torch.quantile(last_unr, 0.05, dim=1, keepdim=True)
        es = torch.where(last_unr < q, last_unr, torch.tensor(float('nan'), device=returns.device))
        es_mean = torch.nanmean(es, dim=1)
        tail_risk = torch.sigmoid(-(es_mean - q.squeeze(1)))
        comb = regime * moment + (1 - regime) * acc + 0.1 * torch.tanh(robust_skew) - 0.1 * torch.tanh(robust_kurt) + 0.1 * rsi_norm
        vol_factor = 0.7 + 0.6 * torch.sigmoid(vol_med / (robust_vol + eps))
        scale = F.softplus((robust_vol - vol_med) * vol_factor)
        sig = torch.exp(-(comb * scale) ** 2) * tail_risk
        sig_list.append(sig)
        wt = 1.0 / (window_mad ** 2 + eps)
        wt_list.append(wt)
    S = torch.stack(sig_list, dim=0)
    W = torch.stack(wt_list, dim=0)
    dyn_w = torch.softmax(-W, dim=0)
    multi = (S * dyn_w).sum(dim=0)
    overall_median = returns.median(dim=1, keepdim=True)[0]
    overall_mad = (returns - overall_median).abs().median(dim=1, keepdim=True)[0] / 0.6745 + eps
    asset_skew = ((returns - overall_median) ** 3).median(dim=1)[0] / (overall_mad.squeeze(1) ** 3 + eps)
    bw = (0.5 + T / 20.0) * (vol_med / (robust_vol + eps)) * (1 + torch.abs(asset_skew))
    t_idx = torch.arange(T, device=returns.device, dtype=returns.dtype)
    center = T - 1
    diff = (t_idx - center).unsqueeze(0)
    bw_exp = bw.unsqueeze(1) + eps
    ep_kernel = torch.clamp(1 - (diff / bw_exp) ** 2, min=0)
    norm_kernel = ep_kernel.sum(dim=1) + eps
    ema = (returns * ep_kernel).sum(dim=1) / norm_kernel
    trend = torch.exp(-(ema - returns[:, -1]) ** 2 * 10.0)
    med_ret = returns[:, -1].median()
    cross = torch.sigmoid(-8.0 * (returns[:, -1] - med_ret) / (returns.std(dim=1) + eps))
    agg = torch.pow(multi * trend * cross + eps, 1.0 / 3.0)
    return torch.sigmoid(10.0 * (agg - 0.5))
```

## Strategy Description

1. **Volatility Regime Detection**

   - Measures recent volatility over a rolling window (up to 20 periods) and benchmarks against the cross-sectional median.
   - Classifies each asset into high/low volatility regimes to adapt weighting and memory parameters.

2. **Adaptive Decay Parameter**

   - Calculates `d_param` by combining standardized volatility deviation and regime flag.
   - Controls the exponential decay rates used when weighing past returns, emphasizing recent observations under high-volatility regimes.

3. **Multi-Window Momentum with Tail Risk**

   - Evaluates momentum across windows from 10 to 60 hours using exponential decay weights to focus on more recent returns.
   - Computes Conditional Value at Risk (5% CVaR) for each window to quantify downside risk.
   - Blends momentum and tail risk via an exponential penalty on extreme moves, then aggregates across windows using inverse-variance weighting.

4. **Gaussian-Weighted Trend Filters**

   - Applies three Gaussian kernels (fast, medium, slow) over the full return history to isolate trend bifurcations.
   - Bandwidths adapt dynamically based on volatility regime and return skewness, capturing asymmetric market conditions.

5. **Cross-Sectional Skew Adjustment**

   - Generates a ranking signal by comparing the latest return to the median of all assets (here only AUDNZD).
   - Scales signal by asset skewness to account for asymmetric return distributions.

6. **Aggregation & Normalization**

   - Multiplies the multi-window momentum, trend-filter score, and cross-sectional rank signals.
   - Applies a cubic root to control dispersion, then transforms via a sigmoid with a steepness factor of 10 to map into [0,1].

7. **Application to AUDNZD Hourly Data**

   - Hourly backtest shows stable equity growth with controlled drawdowns.
   - Captures both mean-reversion in low-volatility periods and trend-following in breakout regimes.
